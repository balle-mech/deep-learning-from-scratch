{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート準備\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from common.functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法\n",
    "\n",
    "数値微分で勾配を計算するには時間がかかるため、誤差逆伝播法を使う  \n",
    "理解するには、「数式」と「計算グラフ」の2つのアプローチが必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算グラフ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算をグラフ（データの形式）で表したもの\n",
    "\n",
    "<img src=\"./スクリーンショット 2025-01-27 18.50.38.png\" width=\"70%\">\n",
    "\n",
    "実際に簡単な問題を解いてみる。\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A((りんご)) -->|100| B((✕))\n",
    "    B -->|200| C((✕))\n",
    "    C -->|220| D((代金))\n",
    "    E((りんごの個数)) -->|2| B\n",
    "    F((消費税)) -->|1.1| C\n",
    "```\n",
    "\n",
    "順伝播：左から右へ  \n",
    "逆伝播：右から左へ\n",
    "\n",
    "### 計算グラフの良いところ\n",
    "\n",
    "- 局所的な計算ができる\n",
    "  - 例）上のグラフで言うと、200円✕消費税の計算をするノードでは、なぜ200円になったかを意識しなくて良い\n",
    "- 途中の計算結果を保持できる\n",
    "- 逆伝播により微分を効率よく計算できる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の例で逆伝播で微分を計算してみる\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[りんご] -->|x = 100| B[✕]\n",
    "    B -->|t = 200| C[✕]\n",
    "    C -->|220| D[代金L]\n",
    "    E[りんごの個数] -->|2| B\n",
    "    F[消費税] -->|1.1| C\n",
    "\n",
    "    D -->|dL/dy = 1| C\n",
    "    C -->|1 ✕ dL/dt = 1.1| B\n",
    "    B -->|1.1 ✕ dt/dx = 2.2| A\n",
    "```\n",
    "\n",
    "図の右から左までの逆伝播で求められるもの  \n",
    "= りんごの値段に関する支払金額の微分  \n",
    "= りんごの少しの値上がりが、最終的な金額がどれくらい上がるか。\n",
    "\n",
    "連鎖律（次の節で説明）を使って、合成関数の微分をだんだんに行っているイメージ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連鎖律"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "連鎖律を説明する前に合成関数を説明します。\n",
    "\n",
    "### 合成関数\n",
    "複数の関数で構成される関数  \n",
    "\n",
    "例： $z=(x+y)^2$  \n",
    "\n",
    "この式は下記のような2つの式で構成される。  \n",
    "$z = t^2$  \n",
    "$t = x + y$  \n",
    "\n",
    "### 連鎖律\n",
    "\n",
    "- 合成関数の微分の性質のこと\n",
    "  - ある合成関数の微分　＝　ある合成関数を構成するそれぞれの関数の微分の積\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}\n",
    "$$\n",
    "\n",
    "### 連鎖律を使って微分を求めてみる\n",
    "\n",
    "$z=(x+y)^2$の微分を、連鎖律を使って求めてみます。  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial t} = 2t\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial t}{\\partial x} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = 2t・1 = 2(x+y)\n",
    "$$\n",
    "\n",
    "### 連鎖律の計算を計算グラフで表してみる\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[ ] -->|x| B[＋]\n",
    "    B -->|t| C[**2]\n",
    "    C -->|z| D[ ]\n",
    "    E[ ] -->|y| B\n",
    "\n",
    "    D -->|dz/dz| C\n",
    "    C -->|dz/dz ✕ dz/dt| B\n",
    "    B -->|dz/dz ✕ dz/dt ✕ dt/dx| A\n",
    "```\n",
    "\n",
    "### まとめ\n",
    "連鎖律には局所的な微分を伝達する原理がある。  \n",
    "計算グラフの逆伝播とやっていることは同じ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加算ノードの逆伝播\n",
    "加算ノードを逆伝播するとどうなるのかについて考える。\n",
    "\n",
    "$$z = x+y$$\n",
    "この式を微分してみると\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = 1\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "となる。  \n",
    "これを計算グラフで書くと、\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[ ] -->|x| C[＋]\n",
    "    B[ ] -->|y| C\n",
    "    C -->|z| D[ ]\n",
    "    D --> E[L]\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "graph RL\n",
    "    A[L] --> B[ ]\n",
    "    B -->|dL/dz| C[＋]\n",
    "    C -->|dL/dz ✕ 1| D[ ]\n",
    "    C -->|dL/dz ✕ 1| E[ ]\n",
    "```\n",
    "\n",
    "上の計算グラフから、dL/dzがそのまま逆伝播されているだけなのがわかる。  \n",
    "\n",
    "実際の加算ノードでは下の計算グラフのように加算ノードの前後のノードで何らかの計算が行われているが、  \n",
    "計算グラフ・連鎖律では局所的な微分を考えればよいので、何らかの計算は考慮しなくて良い。  \n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[何らかの計算] -->|x| C[＋]\n",
    "    B[何らかの計算] -->|y| C\n",
    "    C -->|z| D[何らかの計算]\n",
    "    D --> E[L]\n",
    "```\n",
    "\n",
    "そのため、加算ノードの逆伝播は1を乗算するだけ、つまり、逆伝播で入力された値をそのまま逆伝播するだけと考えて良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 乗算ノードの逆伝播\n",
    "乗算ノードを逆伝播するとどうなるのかについて考える。\n",
    "\n",
    "$$z = xy$$\n",
    "このような乗算の式を微分してみると\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial y} = x\n",
    "$$\n",
    "\n",
    "となる。  \n",
    "\n",
    "つまり、乗算の逆伝播は、**上流の値　✕　順伝播の信号をひっくり返した値**　を下流に流す。  \n",
    "これを計算グラフで書くと、\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[ ] -->|x| C[✕]\n",
    "    B[ ] -->|y| C\n",
    "    C -->|z| D[ ]\n",
    "    D --> E[L]\n",
    "```\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph RL\n",
    "    A[L] --> B[ ]\n",
    "    B -->|dL/dz| C[✕]\n",
    "    C -->|dL/dz ✕ y| D[ ]\n",
    "    C -->|dL/dz ✕ x| E[ ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 乗算ノードと加算ノードの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### レイヤ\n",
    "レイヤとは機能の単位  \n",
    "ニューラルネットワークでいう層のこと\n",
    "\n",
    "乗算ノードと加算ノードをそれぞれ、乗算レイヤーと加算レイヤーとして実装する。  \n",
    "レイヤーを一つのクラスで実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乗算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑　`backward`関数の微分計算では、順伝播入力値をひっくり返して乗算する実装になっている。\n",
    "\n",
    "乗算レイヤを使って、りんごの買い物を実装してみる\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[りんご] -->|x = 100| B[✕]\n",
    "    B -->|t = 200| C[✕]\n",
    "    C -->|220| D[代金L]\n",
    "    E[りんごの個数] -->|2| B\n",
    "    F[消費税] -->|1.1| C\n",
    "\n",
    "    D -->|dL/dy = 1| C\n",
    "    C -->|1 ✕ dL/dt = 1.1| B\n",
    "    B -->|1.1 ✕ dt/dx = 2.2| A\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 220\n",
      "dL/dx(りんごの値段が合計代金にどれくらい影響するか): 2.2\n",
      "dL/dy: 110\n",
      "dL/d消費税: 200\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dL/dx(りんごの値段が合計代金にどれくらい影響するか):\", dapple)\n",
    "print(\"dL/dy:\", int(dapple_num))\n",
    "print(\"dL/d消費税:\", dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑　個数を乗算するレイヤーと、消費税を乗算するレイヤーとで入力値、出力値が異なるため  \n",
    "　　個数と消費税のレイヤーそれぞれのインスタンスを作成している。  \n",
    "　　（局所的な計算を保持できるのが計算グラフ、と前述したが、実際の実装でも保持できている）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑　今回は下のような計算グラフの構成となり、値を保持する必要がないため初期化はしていない。  \n",
    "　　（乗算レイヤーで計算した値を入力して、計算計算結果を出力すれば良い）  \n",
    "　　`pass`は何も行わないという命令\n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    %% --- Apple part ---\n",
    "    A[りんご単価] -->|100| B[✕]\n",
    "    E[個数] -->|2| B\n",
    "    B -->|りんご代200円| J[＋]\n",
    "    B -->|2.2| A\n",
    "    J -->|1.1| B\n",
    "\n",
    "    %% --- Orange part ---\n",
    "    G[みかん単価] -->|150| I[✕]\n",
    "    H[個数] -->|3| I\n",
    "    I -->|みかん代450円| J\n",
    "    I -->|3.3| G\n",
    "\n",
    "    %% --- Summation (Apple + Orange) ---\n",
    "    J -->|小計 = 650| K[✕]\n",
    "    K -->|1.1| J\n",
    "    F[消費税] -->|1.1| K\n",
    "    K -->|最終代金 = 715| D[代金 L]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここまでをまとめると"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 計算グラフは局所的な計算によって構成される\n",
    "- 計算グラフの逆伝播によって各ノードの微分を求めることができる\n",
    "- ノードをレイヤーとして実装し部品のように組み合わせた\n",
    "  - 合成関数の微分の計算を簡単な実装で求めることができた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前節ではりんごの値段の計算等を題材に計算グラフを実装したが、  \n",
    "この節では計算グラフの考え方をニューラルネットワークに適用する。\n",
    "\n",
    "活性化関数を実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLUレイヤ\n",
    "ReLUの数式は下記で表される\n",
    "\n",
    "$$\n",
    "y(x) =\n",
    "\\begin{cases}\n",
    "x & (x > 0) \\\\\n",
    "0 & (x \\leq 0) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "これのxに関するyの微分を計算すると、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x}\n",
    "=\n",
    "\\begin{cases}\n",
    "1 & (x > 0) \\\\\n",
    "0 & (x \\leq 0) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "この数式を計算グラフにあてはめると...  \n",
    "順伝播の入力値が0より大きければ、逆伝播された値をそのまま下流に流す。  \n",
    "順伝播の入力値が0以下であれば、０，つまり下流へ流れる値はなく、信号はそこでストップとなる。  \n",
    "\n",
    "計算グラフで表すと以下のようになる。\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A(( )) -->|x > 0| B((ReLU))\n",
    "    B -->|y| C(( ))\n",
    "    C -->|dL/dy| B\n",
    "    B -->|dL/dy| A\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A(( )) -->|x <= 0| B((ReLU))\n",
    "    B -->|y| C(( ))\n",
    "    C -->|dL/dy| B\n",
    "    B -->|0| A\n",
    "```\n",
    "\n",
    "ReLUレイヤを実装すると、以下のコードになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoidレイヤ\n",
    "シグモイド関数\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affineレイヤ\n",
    "カメラライブラリ OpenCVのアフィン変換と\n",
    "\n",
    "伝播するのが、スカラ値でなく行列になった。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アフィンレイヤのバイアスに対する勾配 $\\frac{\\partial L}{\\partial b}$  \n",
    "バイアスの勾配は、出力の勾配 $\\frac{\\partial L}{\\partial y}$ をそのまま伝播させるだけです。  \n",
    "これは、バイアスが入力に対して直接加算されるためです。\n",
    "\n",
    "具体的には、バイアスの勾配は以下のように計算されます。\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial y_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 重み・バイアスパラメータの微分\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # テンソル対応\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "        [\n",
    "            [1, 2],\n",
    "            [3, 4]\n",
    "        ],\n",
    "        [\n",
    "            [1, 2],\n",
    "            [3, 4]\n",
    "        ]\n",
    "    ])\n",
    "print(x.shape)  # 2*2*2\n",
    "original_x_shape = x.shape\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "# -1は残りの次元数を指定する\n",
    "# つまり、8/2で、x = x.reshape(2, 4)　と同じ\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft-with-Lossレイヤ\n",
    "3章で、softmax関数はニューラルネットワークで使わないと言っていたのは、  \n",
    "あくまで、答えを一つだけ出したい推論時には、スコアの最大値をアフィンレイヤのスコア最大値に着目すればよいから必要ないという話だった。  \n",
    "ニューラルネットワークの学習時には必要となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■交差エントロピー誤差を使う理由\n",
    "softmax関数の損失関数として使うと、逆伝播がきれいな状態になるため（出力と正解の差）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax-with-lossレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチサイズで割ることでデータ一個あたりの誤差が前レイヤへ伝播する  \n",
    "\n",
    "使ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7868317613004097\n",
      "--------------------\n",
      "y-t:\n",
      "[[ 0.01821127  0.24519181 -0.26340309]\n",
      " [ 0.25462853 -0.71859196  0.46396343]]\n",
      "--------------------\n",
      "dx = (y-t)/バッチサイズ:\n",
      " [[ 0.00910564  0.12259591 -0.13170154]\n",
      " [ 0.12731426 -0.35929598  0.23198171]]\n"
     ]
    }
   ],
   "source": [
    "# 入力データと教師データ\n",
    "x = np.array([[0.3, 2.9, 4.0], [0.1, 0.2, 0.7]])\n",
    "t = np.array([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "# SoftmaxWithLossクラスのインスタンスを作成\n",
    "loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "# 順伝播\n",
    "loss = loss_layer.forward(x, t)\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "print('--------------------')\n",
    "print(f'y-t:\\n{loss_layer.y - loss_layer.t}')\n",
    "\n",
    "# 逆伝播\n",
    "dout = 1\n",
    "dx = loss_layer.backward(dout)\n",
    "print('--------------------')\n",
    "print(\"dx = (y-t)/バッチサイズ:\\n\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチサイズで割るところを、あとで数式にしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    # 認識結果を得る処理\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        # ワンホットエンコーディングであったら1次元のクラスラベルの配列に変換する\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # 重みパラメータに対する勾配を数値微分によって求める\n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    # 勾配を誤差逆伝播法によって求める処理\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "レイヤを正しい順番に連結し呼び出すだけで、  \n",
    "簡単にニューラルネットワークを構築できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差逆伝播法の勾配確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.105146256561173e-10\n",
      "b1:1.8790693390806336e-09\n",
      "W2:4.3647966581394145e-09\n",
      "b2:1.4010959351745678e-07\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 各重みの絶対誤差の平均を求める\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print(x[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size:60000\n",
      "0.10308333333333333 0.1023\n",
      "0.9025 0.9057\n",
      "0.9202166666666667 0.9208\n",
      "0.93295 0.9322\n",
      "0.9447166666666666 0.9427\n",
      "0.9509 0.9481\n",
      "0.9565666666666667 0.9524\n",
      "0.9628333333333333 0.9588\n",
      "0.9664166666666667 0.9605\n",
      "0.9685833333333334 0.9626\n",
      "0.9709833333333333 0.9636\n",
      "0.9728 0.9665\n",
      "0.97385 0.9674\n",
      "0.9764333333333334 0.9692\n",
      "0.9773166666666666 0.969\n",
      "0.9783333333333334 0.9694\n",
      "0.9797666666666667 0.9721\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "from common.functions import *  # Importing all functions from the common.functions module\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "print(f'train_size:{train_size}')\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 誤差逆伝播法によって勾配を求める\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
